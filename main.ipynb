{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perkalian dari 2 dan 3 adalah 6\n"
     ]
    }
   ],
   "source": [
    "#program simple calculator\n",
    "operator = str(input('Pilih operator \"penambahan\", \"pengurangan\", \"perkalian\", atau \"pembagian\": '))\n",
    "number1 = int(input(\"Masukkan angka pertama: \"))\n",
    "number2 = int(input(\"Masukkan angka kedua: \"))\n",
    "\n",
    "\n",
    "if operator == 'penambahan':\n",
    "  res = number1 + number2\n",
    "elif operator == 'pengurangan':\n",
    "  res = number1 - number2\n",
    "elif operator == 'perkalian':\n",
    "  res = number1 * number2\n",
    "elif operator == 'pembagian':\n",
    "  res = number1 / number2\n",
    "\n",
    "\n",
    "print(f'{operator} dari {number1} dan {number2} adalah {res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_decompress(url, storage_path, storage_dir):\n",
    "   import os.path\n",
    "   directory = storage_path + \"/\" + storage_dir\n",
    "   zip_file = directory + \".zip\"\n",
    "   a_file = directory + \"/cornell movie-dialogs corpus/README.txt\"\n",
    "   if not os.path.isfile(a_file):\n",
    "       import urllib.request\n",
    "       import zipfile\n",
    "       urllib.request.urlretrieve(url, zip_file)\n",
    "       with zipfile.ZipFile(zip_file, \"r\") as zfh:\n",
    "           zfh.extractall(directory)\n",
    "   return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "def read_conversations(storage_path, storage_dir):\n",
    "   filename = storage_path + \"/\" + storage_dir + \"/cornell movie-dialogs corpus/movie_conversations.txt\"\n",
    "   with open(filename, \"r\", encoding=\"ISO-8859-1\") as fh:\n",
    "       conversations_chunks = [line.split(\" +++$+++ \") for line in fh]\n",
    "   return [re.sub('[[]]', '', el[3].strip()).split(\", \") for el in conversations_chunks]\n",
    "\n",
    "def read_lines(storage_path, storage_dir):\n",
    "   filename = storage_path + \"/\" + storage_dir + \"/cornell movie-dialogs corpus/movie_lines.txt\"\n",
    "   with open(filename, \"r\", encoding=\"ISO-8859-1\") as fh:\n",
    "       lines_chunks = [line.split(\" +++$+++ \") for line in fh]\n",
    "   return {line[0]: line[-1].strip() for line in lines_chunks}\n",
    "\n",
    "def get_tokenized_sequencial_sentences(list_of_lines, line_text):\n",
    "   for line in list_of_lines:\n",
    "       for i in range(len(line) - 1):\n",
    "           yield (line_text[line[i]].split(\" \"), line_text[line[i+1]].split(\" \"))\n",
    "\n",
    "def retrieve_cornell_corpora(storage_path=\"/tmp\", storage_dir=\"cornell_movie_dialogs_corpus\"):\n",
    "   download_and_decompress(\"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\",      \n",
    "                     storage_path,\n",
    "                           storage_dir)\n",
    "   conversations = read_conversations(storage_path, storage_dir)\n",
    "   lines = read_lines(storage_path, storage_dir)\n",
    "   return tuple(zip(*list(get_tokenized_sequencial_sentences(conversations, lines))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'corpora_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcorpora_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcorpora_downloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m retrieve_cornell_corpora\n\u001b[0;32m      3\u001b[0m sen_l1, sen_l2 \u001b[38;5;241m=\u001b[39m retrieve_cornell_corpora()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'corpora_tools'"
     ]
    }
   ],
   "source": [
    "from corpora_tools import *\n",
    "from corpora_downloader import retrieve_cornell_corpora\n",
    "sen_l1, sen_l2 = retrieve_cornell_corpora()\n",
    "print(\"# Two consecutive sentences in a conversation\")\n",
    "print(\"Q:\", sen_l1[0])\n",
    "print(\"A:\", sen_l2[0])\n",
    "print(\"# Corpora length (i.e. number of sentences)\")\n",
    "print(len(sen_l1))\n",
    "assert len(sen_l1) == len(sen_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Two consecutive sentences in a conversation\n",
    "Q: ['Can', 'we', 'make', 'this', 'quick?', '', 'Roxanne', 'Korrine', 'and', 'Andrew', 'Barrett', 'are', 'having', 'an', 'incredibly', 'horrendous', 'public', 'break-', 'up', 'on', 'the', 'quad.', '', 'Again.']\n",
    "A: ['Well,', 'I', 'thought', \"we'd\", 'start', 'with', 'pronunciation,', 'if', \"that's\", 'okay', 'with', 'you.']\n",
    "# Corpora length (i.e. number of sentences)\n",
    "221616\n",
    "\n",
    "clean_sen_l1 = [clean_sentence(s) for s in sen_l1]\n",
    "clean_sen_l2 = [clean_sentence(s) for s in sen_l2]\n",
    "filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(clean_sen_l1, clean_sen_l2)\n",
    "print(\"# Filtered Corpora length (i.e. number of sentences)\")\n",
    "print(len(filt_clean_sen_l1))\n",
    "assert len(filt_clean_sen_l1) == len(filt_clean_sen_l2)\n",
    "\n",
    "# Filtered Corpora length (i.e. number of sentences)\n",
    "140261\n",
    "\n",
    "dict_l1 = create_indexed_dictionary(filt_clean_sen_l1, dict_size=15000, storage_path=\"/tmp/l1_dict.p\")\n",
    "dict_l2 = create_indexed_dictionary(filt_clean_sen_l2, dict_size=15000, storage_path=\"/tmp/l2_dict.p\")\n",
    "idx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\n",
    "idx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\n",
    "print(\"# Same sentences as before, with their dictionary ID\")\n",
    "print(\"Q:\", list(zip(filt_clean_sen_l1[0], idx_sentences_l1[0])))\n",
    "print(\"A:\", list(zip(filt_clean_sen_l2[0], idx_sentences_l2[0])))\n",
    "\n",
    "[sentences_to_indexes] Did not find 16823 words\n",
    "[sentences_to_indexes] Did not find 16649 words\n",
    "# Same sentences as before, with their dictionary ID\n",
    "Q: [('well', 68), (',', 8), ('i', 9), ('thought', 141), ('we', 23), (\"'\", 5), ('d', 83), ('start', 370), ('with', 46), ('pronunciation', 3), (',', 8), ('if', 78), ('that', 18), (\"'\", 5), ('s', 12), ('okay', 92), ('with', 46), ('you', 7), ('.', 4)]\n",
    "A: [('not', 31), ('the', 10), ('hacking', 7309), ('and', 23), ('gagging', 8761), ('and', 23), ('spitting', 6354), ('part', 437), ('.', 4), ('please', 145), ('.', 4)]\n",
    "\n",
    "data_set = prepare_sentences(idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2)\n",
    "print(\"# Prepared minibatch with paddings and extra stuff\")\n",
    "print(\"Q:\", data_set[0][0])\n",
    "print(\"A:\", data_set[0][1])\n",
    "print(\"# The sentence pass from X to Y tokens\")\n",
    "print(\"Q:\", len(idx_sentences_l1[0]), \"->\", len(data_set[0][0]))\n",
    "print(\"A:\", len(idx_sentences_l2[0]), \"->\", len(data_set[0][1]))\n",
    "\n",
    "# Prepared minibatch with paddings and extra stuff\n",
    "Q: [0, 68, 8, 9, 141, 23, 5, 83, 370, 46, 3, 8, 78, 18, 5, 12, 92, 46, 7, 4]\n",
    "A: [1, 31, 10, 7309, 23, 8761, 23, 6354, 437, 4, 145, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# The sentence pass from X to Y tokens\n",
    "Q: 19 -> 20\n",
    "A: 11 -> 22\n",
    "\n",
    "def build_dataset(use_stored_dictionary=False):\n",
    "   sen_l1, sen_l2 = retrieve_cornell_corpora()\n",
    "   clean_sen_l1 = [clean_sentence(s) for s in sen_l1][:30000] ### OTHERWISE IT DOES NOT RUN ON MY LAPTOP\n",
    "   clean_sen_l2 = [clean_sentence(s) for s in sen_l2][:30000] ### OTHERWISE IT DOES NOT RUN ON MY LAPTOP\n",
    "   filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(clean_sen_l1, clean_sen_l2, max_len=10)\n",
    "   if not use_stored_dictionary:\n",
    "       dict_l1 = create_indexed_dictionary(filt_clean_sen_l1, dict_size=10000, storage_path=path_l1_dict)\n",
    "       dict_l2 = create_indexed_dictionary(filt_clean_sen_l2, dict_size=10000, storage_path=path_l2_dict)\n",
    "   else:\n",
    "       dict_l1 = pickle.load(open(path_l1_dict, \"rb\"))\n",
    "       dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n",
    "   dict_l1_length = len(dict_l1)\n",
    "   dict_l2_length = len(dict_l2)\n",
    "   idx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\n",
    "   idx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\n",
    "   max_length_l1 = extract_max_length(idx_sentences_l1)\n",
    "   max_length_l2 = extract_max_length(idx_sentences_l2)\n",
    "   data_set = prepare_sentences(idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2)\n",
    "   return (filt_clean_sen_l1, filt_clean_sen_l2), \n",
    "           data_set, \n",
    "           (max_length_l1, max_length_l2), \n",
    "           (dict_l1_length, dict_l2_length)\n",
    "\n",
    "[sentences_to_indexes] Did not find 0 words\n",
    "[sentences_to_indexes] Did not find 0 words\n",
    "global step 100 learning rate 1.0 step-time 7.708967611789704 perplexity 444.90090078460474\n",
    "eval: perplexity 57.442316329639176\n",
    "global step 200 learning rate 0.990234375 step-time 7.700247814655302 perplexity 48.8545568311572\n",
    "eval: perplexity 42.190180314697045\n",
    "global step 300 learning rate 0.98046875 step-time 7.69800933599472 perplexity 41.620538109894945\n",
    "eval: perplexity 31.291903031786116\n",
    "...\n",
    "...\n",
    "...\n",
    "global step 2400 learning rate 0.79833984375 step-time 7.686293318271639 perplexity 3.7086356605442767\n",
    "eval: perplexity 2.8348589631663046\n",
    "global step 2500 learning rate 0.79052734375 step-time 7.689657487869262 perplexity 3.211876894960698\n",
    "eval: perplexity 2.973809378544393\n",
    "global step 2600 learning rate 0.78271484375 step-time 7.690396382808681 perplexity 2.878854805600354\n",
    "eval: perplexity 2.563583924617356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import data_utils\n",
    "from corpora_tools import clean_sentence, sentences_to_indexes, prepare_sentences\n",
    "from train_chatbot import get_seq2seq_model, path_l1_dict, path_l2_dict\n",
    "model_dir = \"/home/abc/chat/chatbot_model\"\n",
    "def prepare_sentence(sentence, dict_l1, max_length):\n",
    "   sents = [sentence.split(\" \")]\n",
    "   clean_sen_l1 = [clean_sentence(s) for s in sents]\n",
    "   idx_sentences_l1 = sentences_to_indexes(clean_sen_l1, dict_l1)\n",
    "   data_set = prepare_sentences(idx_sentences_l1, [[]], max_length, max_length)\n",
    "   sentences = (clean_sen_l1, [[]])\n",
    "   return sentences, data_set\n",
    "\n",
    "def decode(data_set):\n",
    "with tf.Session() as sess:\n",
    "   model = get_seq2seq_model(sess, True, dict_lengths, max_sentence_lengths, model_dir)\n",
    "   model.batch_size = 1\n",
    "   bucket = 0\n",
    "   encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "     {bucket: [(data_set[0][0], [])]}, bucket)\n",
    "   _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                   target_weights, bucket, True)\n",
    "   outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "   if data_utils.EOS_ID in outputs:\n",
    "       outputs = outputs[1:outputs.index(data_utils.EOS_ID)]\n",
    "tf.reset_default_graph()\n",
    "return \" \".join([tf.compat.as_str(inv_dict_l2[output]) for output in outputs])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   dict_l1 = pickle.load(open(path_l1_dict, \"rb\"))\n",
    "   dict_l1_length = len(dict_l1)\n",
    "   dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n",
    "   dict_l2_length = len(dict_l2)\n",
    "   inv_dict_l2 = {v: k for k, v in dict_l2.items()}\n",
    "   max_lengths = 10\n",
    "   dict_lengths = (dict_l1_length, dict_l2_length)\n",
    "   max_sentence_lengths = (max_lengths, max_lengths)\n",
    "   from bottle import route, run, request\n",
    "   @route('/api')\n",
    "   def api():\n",
    "       in_sentence = request.query.sentence\n",
    "     _, data_set = prepare_sentence(in_sentence, dict_l1, max_lengths)\n",
    "       resp = [{\"in\": in_sentence, \"out\": decode(data_set)}]\n",
    "       return dict(data=resp)\n",
    "   run(host='127.0.0.1', port=8080, reloader=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
